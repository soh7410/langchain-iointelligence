"""IOIntelligenceChatModel implementation for LangChain."""

import os
from typing import Any, Dict, List, Optional, Iterator
import requests
from dotenv import load_dotenv
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage
from langchain_core.outputs import ChatGeneration, ChatResult
from langchain_core.exceptions import OutputParserException as GenerationError
from langchain_core.callbacks.manager import CallbackManagerForLLMRun

# Load environment variables from .env file
load_dotenv()


class IOIntelligenceChatModel(BaseChatModel):
    """LangChain ChatModel wrapper for io Intelligence API."""
    
    # Declare all fields that will be used
    io_api_key: str = ""
    io_api_url: str = ""
    model: str = "meta-llama/Llama-3.3-70B-Instruct"
    max_tokens: int = 1000
    temperature: float = 0.7
    timeout: int = 30
    max_retries: int = 3
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        api_url: Optional[str] = None,
        model: str = "meta-llama/Llama-3.3-70B-Instruct",
        max_tokens: int = 1000,
        temperature: float = 0.7,
        timeout: int = 30,
        max_retries: int = 3,
        **kwargs
    ):
        """Initialize IOIntelligenceChatModel.
        
        Args:
            api_key: io Intelligence API key (optional, defaults to IO_API_KEY env var)
            api_url: io Intelligence API URL (optional, defaults to IO_API_URL env var)
            model: Model name to use (default: "meta-llama/Llama-3.3-70B-Instruct")
            max_tokens: Maximum tokens to generate (default: 1000)
            temperature: Temperature for generation (default: 0.7)
            timeout: Request timeout in seconds (default: 30)
            max_retries: Maximum number of retries (default: 3)
        """
        # Extract and set API credentials
        api_key = api_key or os.getenv('IO_API_KEY')
        api_url = api_url or os.getenv('IO_API_URL')
        
        if not api_key:
            raise ValueError("IO_API_KEY must be provided either as parameter or environment variable")
        if not api_url:
            raise ValueError("IO_API_URL must be provided either as parameter or environment variable")
        
        # Set field values explicitly
        kwargs.update({
            'io_api_key': api_key,
            'io_api_url': api_url,
            'model': model,
            'max_tokens': max_tokens,
            'temperature': temperature,
            'timeout': timeout,
            'max_retries': max_retries
        })
            
        super().__init__(**kwargs)
    
    @property
    def _llm_type(self) -> str:
        """Return identifier of LLM type."""
        return "io_intelligence_chat"
    
    def _convert_messages_to_api_format(self, messages: List[BaseMessage]) -> List[Dict[str, str]]:
        """Convert LangChain messages to API format."""
        api_messages = []
        
        for message in messages:
            if isinstance(message, HumanMessage):
                api_messages.append({"role": "user", "content": message.content})
            elif isinstance(message, AIMessage):
                api_messages.append({"role": "assistant", "content": message.content})
            elif isinstance(message, SystemMessage):
                api_messages.append({"role": "system", "content": message.content})
            else:
                # Generic message - default to user
                api_messages.append({"role": "user", "content": message.content})
        
        return api_messages
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Run the LLM on the given messages.
        
        Args:
            messages: The messages to pass into the model.
            stop: Optional list of stop words to use when generating.
            run_manager: Optional callback manager.
            **kwargs: Additional keyword arguments.
            
        Returns:
            The ChatResult generated by the model.
            
        Raises:
            GenerationError: If the API request fails.
        """
        headers = {
            "Authorization": f"Bearer {self.io_api_key}",
            "Content-Type": "application/json",
        }
        
        # Convert messages to API format
        api_messages = self._convert_messages_to_api_format(messages)
        
        # io Intelligence API uses OpenAI-compatible format
        data = {
            "model": self.model,
            "messages": api_messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        
        # Add stop words if provided
        if stop:
            data["stop"] = stop
            
        # Add any additional kwargs
        data.update(kwargs)
        
        try:
            response = requests.post(
                self.io_api_url,
                headers=headers,
                json=data,
                timeout=self.timeout
            )
            response.raise_for_status()
            
            # Parse response - supports both Chat and Completion formats
            response_data = response.json()
            
            if "choices" not in response_data or not response_data["choices"]:
                raise GenerationError("No choices in API response")
                
            choice = response_data["choices"][0]
            
            # Chat format: choices[0].message.content
            if "message" in choice and "content" in choice["message"]:
                content = choice["message"]["content"]
            # Completion format: choices[0].text
            elif "text" in choice:
                content = choice["text"]
            else:
                raise GenerationError("Unsupported response schema - expected 'message.content' or 'text' in choices")
            
            # Create AIMessage with the response
            message = AIMessage(content=content)
            
            # Extract usage information if available
            usage_data = response_data.get("usage", {})
            
            generation = ChatGeneration(
                message=message,
                generation_info={
                    "model": self.model,
                    "usage": usage_data,
                    "finish_reason": choice.get("finish_reason"),
                }
            )
            
            return ChatResult(generations=[generation])
            
        except requests.exceptions.RequestException as e:
            raise GenerationError(f"API request failed: {str(e)}")
        except (KeyError, IndexError) as e:
            raise GenerationError(f"Invalid API response format: {str(e)}")
    
    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Return a dictionary of identifying parameters."""
        return {
            "model": self.model,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "timeout": self.timeout,
            "max_retries": self.max_retries,
        }
    
    # Streaming support (optional - can be added later)
    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[ChatGeneration]:
        """Stream the LLM on the given messages."""
        # For now, just use the non-streaming version
        # This can be enhanced later with actual streaming support
        result = self._generate(messages, stop, run_manager, **kwargs)
        yield result.generations[0]


# Alias for backward compatibility
IOIntelligenceChat = IOIntelligenceChatModel
