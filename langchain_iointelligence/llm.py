"""IOIntelligenceLLM implementation for LangChain."""

import os
from typing import Any, Dict, List, Optional

import requests
from dotenv import load_dotenv
from langchain_core.exceptions import OutputParserException as GenerationError
from langchain_core.language_models.llms import LLM

# Load environment variables from .env file
load_dotenv()


class IOIntelligenceLLM(LLM):
    """LangChain LLM wrapper for io Intelligence API."""

    # Declare all fields that will be used
    io_api_key: str = ""
    io_api_url: str = ""
    model: str = "meta-llama/Llama-3.3-70B-Instruct"
    max_tokens: int = 1000
    temperature: float = 0.7

    def __init__(self, api_key: Optional[str] = None, api_url: Optional[str] = None, **kwargs):
        """Initialize IOIntelligenceLLM.

        Args:
            api_key: io Intelligence API key (optional, defaults to IO_API_KEY env var)
            api_url: io Intelligence API URL (optional, defaults to IO_API_URL env var)
            model: Model name to use (default: "meta-llama/Llama-3.3-70B-Instruct")
            max_tokens: Maximum tokens to generate (default: 1000)
            temperature: Temperature for generation (default: 0.7)
        """
        # Extract and set API credentials
        api_key = api_key or os.getenv("IO_API_KEY")
        api_url = api_url or os.getenv("IO_API_URL")

        if not api_key:
            raise ValueError(
                "IO_API_KEY must be provided either as parameter or environment variable"
            )
        if not api_url:
            raise ValueError(
                "IO_API_URL must be provided either as parameter or environment variable"
            )

        # Set field values explicitly
        kwargs["io_api_key"] = api_key
        kwargs["io_api_url"] = api_url

        super().__init__(**kwargs)

    @property
    def _llm_type(self) -> str:
        """Return identifier of LLM type."""
        return "io_intelligence"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[Any] = None,
        **kwargs: Any,
    ) -> str:
        """Run the LLM on the given prompt and input.

        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.
            run_manager: Optional callback manager.
            **kwargs: Additional keyword arguments.

        Returns:
            The string generated by the model.

        Raises:
            GenerationError: If the API request fails.
        """
        headers = {
            "Authorization": f"Bearer {self.io_api_key}",
            "Content-Type": "application/json",
        }

        # io Intelligence API uses OpenAI-compatible format
        data = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }

        # Add stop words if provided
        if stop:
            data["stop"] = stop

        # Add any additional kwargs
        data.update(kwargs)

        try:
            response = requests.post(self.io_api_url, headers=headers, json=data, timeout=30)
            response.raise_for_status()

            # Parse response - supports both Chat and Completion formats
            response_data = response.json()

            if "choices" not in response_data or not response_data["choices"]:
                raise GenerationError("No choices in API response")

            choice = response_data["choices"][0]

            # Chat format: choices[0].message.content
            if "message" in choice and "content" in choice["message"]:
                return choice["message"]["content"]

            # Completion format: choices[0].text
            if "text" in choice:
                return choice["text"]

            raise GenerationError(
                "Unsupported response schema - expected 'message.content' or 'text' in choices"
            )

        except requests.exceptions.RequestException as e:
            raise GenerationError(f"API request failed: {str(e)}")
        except (KeyError, IndexError) as e:
            raise GenerationError(f"Invalid API response format: {str(e)}")

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Return a dictionary of identifying parameters."""
        return {
            "model": self.model,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
        }
